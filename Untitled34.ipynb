{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained VGG16 model (you can also use ResNet, Inception, etc.)\n",
    "image_model = VGG16(include_top=True, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-2].output\n",
    "image_features_extract_model = Model(inputs=new_input, outputs=hidden_layer)\n",
    "\n",
    "# Define the image preprocessing function\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Define the maximum caption length and vocabulary size\n",
    "max_caption_length = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# Load your dataset with image paths and corresponding captions\n",
    "\n",
    "# Extract image features and preprocess captions\n",
    "image_features = {}\n",
    "captions = []\n",
    "\n",
    "# Loop over your dataset\n",
    "for image_path, caption in dataset:\n",
    "    img = preprocess_image(image_path)\n",
    "    features = image_features_extract_model.predict(img)\n",
    "    image_features[image_path] = features[0]\n",
    "    captions.append(caption)\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(captions)\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Prepare the data for training\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "        input_seq = sequence[:i]\n",
    "        output_seq = sequence[i]\n",
    "        X.append(input_seq)\n",
    "        y.append(output_seq)\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Build the caption generation model using LSTM\n",
    "input_image_features = Input(shape=(4096,))\n",
    "image_embedding = Dense(256, activation=\"relu\")(input_image_features)\n",
    "input_caption = Input(shape=(max_sequence_length,))\n",
    "caption_embedding = Embedding(input_dim=vocab_size, output_dim=256)(input_caption)\n",
    "decoder_input = Add()([image_embedding, caption_embedding])\n",
    "lstm = LSTM(256)(decoder_input)\n",
    "output = Dense(vocab_size, activation=\"softmax\")(lstm)\n",
    "\n",
    "captioning_model = Model(inputs=[input_image_features, input_caption], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "captioning_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train the model with X and y\n",
    "captioning_model.fit([image_features, X], y, batch_size=32, epochs=10)\n",
    "\n",
    "# To generate captions for a new image, you can use the trained model\n",
    "new_image = preprocess_image(\"path_to_new_image.jpg\")\n",
    "new_features = image_features_extract_model.predict(new_image)\n",
    "input_caption = [\"<OOV>\"]\n",
    "for i in range(max_caption_length):\n",
    "    sequence = tokenizer.texts_to_sequences(input_caption)\n",
    "    sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "    next_word_index = np.argmax(captioning_model.predict([new_features, sequence]))\n",
    "    next_word = tokenizer.index_word[next_word_index]\n",
    "    if next_word == \"<OOV>\" or next_word is None:\n",
    "        break\n",
    "    input_caption.append(next_word)\n",
    "\n",
    "caption = \" \".join(input_caption[1:])\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45741869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
